{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOK+Oy554m4Wl5UdqkqIMWd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nishant-Ramakuru/Inference-based-GNNS/blob/main/Synthdata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkXyhYlpyfkW"
      },
      "outputs": [],
      "source": [
        "class SmallSynthData(Dataset):\n",
        "    def __init__(self, data_path, mode, params):\n",
        "        self.mode = mode\n",
        "        self.data_path = data_path\n",
        "        if self.mode == 'train':\n",
        "            path = os.path.join(data_path, 'train_feats')\n",
        "            edge_path = os.path.join(data_path, 'train_edges')\n",
        "        elif self.mode == 'val':\n",
        "            path = os.path.join(data_path, 'val_feats')\n",
        "            edge_path = os.path.join(data_path, 'val_edges')\n",
        "        elif self.mode == 'test':\n",
        "            path = os.path.join(data_path, 'test_feats')\n",
        "            edge_path = os.path.join(data_path, 'test_edges')\n",
        "        self.feats = torch.load(path)\n",
        "        self.edges = torch.load(edge_path)\n",
        "        self.same_norm = params['same_data_norm']\n",
        "        self.no_norm = params['no_data_norm']\n",
        "        if not self.no_norm:\n",
        "            self._normalize_data()\n",
        "\n",
        "    def _normalize_data(self):\n",
        "        train_data = torch.load(os.path.join(self.data_path, 'train_feats'))\n",
        "        if self.same_norm:\n",
        "            self.feat_max = train_data.max()\n",
        "            self.feat_min = train_data.min()\n",
        "            self.feats = (self.feats - self.feat_min)*2/(self.feat_max-self.feat_min) - 1\n",
        "        else:\n",
        "            self.loc_max = train_data[:, :, :, :2].max()\n",
        "            self.loc_min = train_data[:, :, :, :2].min()\n",
        "            self.vel_max = train_data[:, :, :, 2:].max()\n",
        "            self.vel_min = train_data[:, :, :, 2:].min()\n",
        "            self.feats[:,:,:, :2] = (self.feats[:,:,:,:2]-self.loc_min)*2/(self.loc_max - self.loc_min) - 1\n",
        "            self.feats[:,:,:,2:] = (self.feats[:,:,:,2:]-self.vel_min)*2/(self.vel_max-self.vel_min)-1\n",
        "\n",
        "    def unnormalize(self, data):\n",
        "        if self.no_norm:\n",
        "            return data\n",
        "        elif self.same_norm:\n",
        "            return (data + 1) * (self.feat_max - self.feat_min) / 2. + self.feat_min\n",
        "        else:\n",
        "            result1 = (data[:, :, :, :2] + 1) * (self.loc_max - self.loc_min) / 2. + self.loc_min\n",
        "            result2 = (data[:, :, :, 2:] + 1) * (self.vel_max - self.vel_min) / 2. + self.vel_min\n",
        "            return np.concatenate([result1, result2], axis=-1)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {'inputs': self.feats[idx], 'edges':self.edges[idx]}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.feats)\n",
        "        \n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    params['num_train'] = 100\n",
        "    params['num_val'] = 100\n",
        "    params['num_test'] = 100\n",
        "    params['num_time_steps'] = 10\n",
        "    params['pull_factor'] = 0.1\n",
        "    params['push_factor'] = 0.05\n",
        "\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    all_data = []\n",
        "    all_edges = []\n",
        "    num_sims = params['num_train'] + params['num_val'] + params['num_test']\n",
        "    flip_count = 0\n",
        "    total_steps = 0\n",
        "    for sim in range(num_sims):\n",
        "        p1_loc = np.random.uniform(-2, -1, size=(2))\n",
        "        p1_vel = np.random.uniform(0.05, 0.1, size=(2))\n",
        "        p2_loc = np.random.uniform(1, 2, size=(2))\n",
        "        p2_vel = np.random.uniform(-0.05, -0.1, size=(2))\n",
        "        p3_loc = np.random.uniform(-1, 1, size=(2))\n",
        "        p3_vel = np.random.uniform(-0.05, 0.05, size=(2))\n",
        "\n",
        "        current_feats = []\n",
        "        current_edges = []\n",
        "        for time_step in range(params['num_time_steps']):\n",
        "            current_edge = np.array([0,0,0,0,0,0])\n",
        "            current_edges.append(current_edge)\n",
        "            if np.linalg.norm(p3_loc - p1_loc) < 1:\n",
        "                norm = np.linalg.norm(p3_loc - p1_loc)\n",
        "                coef = 1 - norm\n",
        "                dir_1 = (p3_loc - p1_loc)/norm\n",
        "                p3_vel += params['push_factor']*coef*dir_1\n",
        "                current_edge[1] = 1\n",
        "            if np.linalg.norm(p3_loc - p2_loc) < 1:\n",
        "                norm = np.linalg.norm(p3_loc - p2_loc)\n",
        "                coef = 1 - norm\n",
        "                dir_2 = (p3_loc - p2_loc)/norm\n",
        "                p3_vel += params['push_factor']*coef*dir_2\n",
        "                current_edge[3] = 1\n",
        "\n",
        "            p1_loc += p1_vel\n",
        "            p2_loc += p2_vel\n",
        "            p3_loc += p3_vel\n",
        "            p1_feat = np.concatenate([p1_loc, p1_vel])\n",
        "            p2_feat = np.concatenate([p2_loc, p2_vel])\n",
        "            p3_feat = np.concatenate([p3_loc, p3_vel])\n",
        "            new_feat = np.stack([p1_feat, p2_feat, p3_feat])\n",
        "            current_feats.append(new_feat)\n",
        "        all_data.append(np.stack(current_feats))\n",
        "        all_edges.append(np.stack(current_edges))\n",
        "\n",
        "all_data = np.stack(all_data)\n",
        "train_data = torch.FloatTensor(all_data[:params['num_train']])\n",
        "val_data = torch.FloatTensor(all_data[params['num_train']:params['num_train']+params['num_val']])\n",
        "test_data = torch.FloatTensor(all_data[params['num_train']+params['num_val']:])\n",
        "params['output_dir'] = ''\n",
        "train_path = os.path.join(params['output_dir'], 'train_feats')\n",
        "torch.save(train_data, train_path)\n",
        "val_path = os.path.join(params['output_dir'], 'val_feats')\n",
        "torch.save(val_data, val_path)\n",
        "test_path = os.path.join(params['output_dir'], 'test_feats')\n",
        "torch.save(test_data, test_path)\n",
        "\n",
        "train_edges = torch.FloatTensor(all_edges[:params['num_train']])\n",
        "val_edges = torch.FloatTensor(all_edges[params['num_train']:params['num_train']+params['num_val']])\n",
        "test_edges = torch.FloatTensor(all_edges[params['num_train']+params['num_val']:])\n",
        "train_path = os.path.join(params['output_dir'], 'train_edges')\n",
        "torch.save(train_edges, train_path)\n",
        "val_path = os.path.join(params['output_dir'], 'val_edges')\n",
        "torch.save(val_edges, val_path)\n",
        "test_path = os.path.join(params['output_dir'], 'test_edges')\n",
        "torch.save(test_edges, test_path)\n",
        "\n",
        "train_data = SmallSynthData('', 'train', params)\n",
        "val_data = SmallSynthData('', 'val', params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.load('train_feats')\n",
        "train_data_loader = DataLoader(train_data, batch_size=1, shuffle=True, drop_last=False)"
      ],
      "metadata": {
        "id": "7KL-a3CqypOh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}